План семинара:
1. Что такое линейный классификатор, геометричекский смысл вектора весов, возможность получать не только метку класса, но и меру уверенности в ней.
2. Решение многоклассовой задачи (one vs all, each vs each).
3. Margin. Loss functiuon и различные ее апроксимации (есть ссылка на график). 
4. Loss function for perceptron + log loss.
5. GD + SGD + параметры: длина шага (константная и нет), критерий выхода (tolerance), инициализация, рандомный порядок, находим только локальный оптимум. Тут пришлось подробно объяснять, ибо слово градиент очень мало кто вспомнил. 
6. Пример: SGD для персептрона (правило Хэбба). Полезно для понимания предыдущего пункта и для дз (им надо будет дома формулы выводить для логрегрессии).
7. (Частично не успела) Регуляризация: зачем вообще (есть пример в начале ноутбука для log loss), l1 и l2 - разница, отбор признаков (есть ссылка на график). 
8. Аналогия максимизации правдоподобия и логистической регрессии.

Важно для дз:
1. Сказать про правило Хэбба.
2. Рассказать пункт 8.
3. Объяснить геометрический смысл вектора весов.
4. Сказать, что нужно добавлять константный признак к данным.
5. Сказать про регуляризацию в общем.
